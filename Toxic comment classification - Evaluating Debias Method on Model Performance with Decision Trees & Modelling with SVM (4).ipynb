{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d512d513",
   "metadata": {},
   "source": [
    "**Toxic comment classification - Evaluate Model Performance with the Debias Method with Decision Trees, Modelling with SVM**\n",
    "\n",
    "- Apply TF-IDF, split datasets, train DecisionTree, evaluate dibias impact.\n",
    "- Apply word embedding, split datasets, train DecisionTree, compare results.\n",
    "- Train SVM model on debiased dataset (df_train2) with TF-IDF and Word-Embedding features.\n",
    "- Hyperparameter tuning best models from DT and SVM, then evaluation against real-world data (test dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf60358",
   "metadata": {},
   "source": [
    "<div style=\"border-top: 7px solid #800080; animation: sparkling 2s linear infinite;\"></div>\n",
    "\n",
    "<style>\n",
    "@keyframes sparkling {\n",
    "  0% { background-position: 0 0; }\n",
    "  100% { background-position: 100% 0; }\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2142f4e4",
   "metadata": {},
   "source": [
    "**Part 1**\n",
    "\n",
    "- Apply TF-IDF feature representation method.\n",
    "- Split both the original dataset and the dataset which has the sensitive words masked, into training and test sets.\n",
    "- Train both datasets with DesicionTree, evaluate the dibias method's impact and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b3f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf762139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "df_train1 = pd.read_csv('processed_train_data.csv')  # Original data without marking sensitive words\n",
    "df_train2 = pd.read_csv('ready_train.csv')  # Data with sensitive words marking\n",
    "\n",
    "# Vectorize lemmas for dataset 1\n",
    "vectorizer = TfidfVectorizer()\n",
    "X1 = vectorizer.fit_transform(df_train1['lemmas'])\n",
    "y1 = df_train1[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "\n",
    "# Split data into train and test sets for dataset 1\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.3, random_state=42)\n",
    "\n",
    "# Vectorize lemmas for dataset 2\n",
    "X2 = vectorizer.fit_transform(df_train2['lemmas'])\n",
    "y2 = df_train2[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "\n",
    "# Split data into train and test sets for dataset 2\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec4d7b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32450, 57644), (32450, 51860))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.shape, X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "025a64b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train the Decision Tree classifier\n",
    "dt = DecisionTreeClassifier(class_weight='balanced', random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aa34e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for dataset 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.74      0.74      4577\n",
      "           1       0.22      0.41      0.28       496\n",
      "           2       0.59      0.71      0.65      2531\n",
      "           3       0.23      0.41      0.29       144\n",
      "           4       0.52      0.64      0.57      2390\n",
      "           5       0.24      0.42      0.31       413\n",
      "\n",
      "   micro avg       0.57      0.68      0.62     10551\n",
      "   macro avg       0.42      0.55      0.47     10551\n",
      "weighted avg       0.61      0.68      0.64     10551\n",
      " samples avg       0.28      0.32      0.27     10551\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate model for dataset 1\n",
    "dt.fit(X_train1, y_train1)\n",
    "# Predict labels for the test set\n",
    "y_pred1 = dt.predict(X_test1)\n",
    "# Evaluate the model\n",
    "print(\"Classification report for dataset 1:\")\n",
    "print(classification_report(y_test1, y_pred1, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e93d623d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for dataset 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.75      0.75      4577\n",
      "           1       0.19      0.50      0.27       496\n",
      "           2       0.60      0.75      0.67      2531\n",
      "           3       0.17      0.36      0.23       144\n",
      "           4       0.50      0.66      0.57      2390\n",
      "           5       0.10      0.30      0.14       413\n",
      "\n",
      "   micro avg       0.53      0.69      0.60     10551\n",
      "   macro avg       0.38      0.55      0.44     10551\n",
      "weighted avg       0.59      0.69      0.63     10551\n",
      " samples avg       0.27      0.33      0.28     10551\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate model for dataset 2\n",
    "dt.fit(X_train2, y_train2)\n",
    "# Predict labels for the test set\n",
    "y_pred2 = dt.predict(X_test2)\n",
    "# Evaluate the model\n",
    "print(\"Classification report for dataset 2:\")\n",
    "print(classification_report(y_test2, y_pred2, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ac5293",
   "metadata": {},
   "source": [
    "<div style=\"border-top: 7px solid #800080; animation: sparkling 2s linear infinite;\"></div>\n",
    "\n",
    "<style>\n",
    "@keyframes sparkling {\n",
    "  0% { background-position: 0 0; }\n",
    "  100% { background-position: 100% 0; }\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a927d48e",
   "metadata": {},
   "source": [
    "**Part 2**\n",
    "\n",
    "- Apply word embedding feature representation method.\n",
    "- Split both the original dataset and the dataset which has the sensitive words masked, into training and test sets.\n",
    "- Train both datasets with DesicionTree and compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e52e41c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7472f5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate and normalise word embeddings \n",
    "def generate_word_embeddings(df):\n",
    "    embeddings = []\n",
    "    for tokens in df[\"token_nonstop\"]:\n",
    "        sentence_embedding = np.zeros(nlp.vocab.vectors_length)  \n",
    "        word_count = 0  \n",
    "        for token in tokens:\n",
    "            if token in nlp.vocab:\n",
    "                word_embedding = nlp.vocab[token].vector \n",
    "                sentence_embedding += word_embedding \n",
    "                word_count += 1  \n",
    "        if word_count > 0:\n",
    "            sentence_embedding /= word_count \n",
    "        embeddings.append(sentence_embedding)\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d73c7104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix X: (32450, 300)\n"
     ]
    }
   ],
   "source": [
    "# Generate word embeddings for token_nonstop in df_train1\n",
    "X_1 = generate_word_embeddings(df_train1)\n",
    "# Check the shape of the feature matrix\n",
    "print(\"Shape of feature matrix X:\", X_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4344f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label = df_train1[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']] # same for both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16528b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets for dataset 1\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_label, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db66ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train the Decision Tree classifier\n",
    "dt_1 = DecisionTreeClassifier(class_weight='balanced', random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2ada048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for dataset 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.61      0.59      4577\n",
      "           1       0.19      0.23      0.21       496\n",
      "           2       0.43      0.47      0.45      2531\n",
      "           3       0.04      0.06      0.05       144\n",
      "           4       0.37      0.41      0.39      2390\n",
      "           5       0.10      0.13      0.12       413\n",
      "\n",
      "   micro avg       0.44      0.49      0.46     10551\n",
      "   macro avg       0.29      0.32      0.30     10551\n",
      "weighted avg       0.45      0.49      0.47     10551\n",
      " samples avg       0.24      0.25      0.22     10551\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate model for dataset 1\n",
    "dt_1.fit(X_train_1, y_train_1)\n",
    "# Predict labels for the test set\n",
    "y_pred_1 = dt_1.predict(X_test_1)\n",
    "# Evaluate the model\n",
    "print(\"Classification report for dataset 1:\")\n",
    "print(classification_report(y_test_1, y_pred_1, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f452acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix X: (32450, 300)\n"
     ]
    }
   ],
   "source": [
    "# Generate word embeddings for token_nonstop in df_train2\n",
    "X_2 = generate_word_embeddings(df_train2)\n",
    "# Check the shape of the feature matrix\n",
    "print(\"Shape of feature matrix X:\", X_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddfb983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets for dataset 2\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_label, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb2f7b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for dataset 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.62      0.60      4577\n",
      "           1       0.21      0.24      0.22       496\n",
      "           2       0.43      0.49      0.46      2531\n",
      "           3       0.08      0.11      0.10       144\n",
      "           4       0.38      0.43      0.40      2390\n",
      "           5       0.10      0.13      0.12       413\n",
      "\n",
      "   micro avg       0.45      0.50      0.47     10551\n",
      "   macro avg       0.30      0.34      0.32     10551\n",
      "weighted avg       0.46      0.50      0.48     10551\n",
      " samples avg       0.24      0.25      0.22     10551\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate model for dataset 2\n",
    "dt_1.fit(X_train_2, y_train_2)\n",
    "# Predict labels for the test set\n",
    "y_pred_2 = dt_1.predict(X_test_2)\n",
    "# Evaluate the model\n",
    "print(\"Classification report for dataset 2:\")\n",
    "print(classification_report(y_test_2, y_pred_2, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5200674",
   "metadata": {},
   "source": [
    "**In comparison to the previous results, TF-IDF and Word Embedding methods were used for feature representation for both datasets and then trained with a decision tree model. The results do not show too much difference in model performance. This means that masking sensitive words can reduce bias on the one hand, and does not affect model performance on the other. This means that the masked dataset will be used in the project from now on.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b26f0a",
   "metadata": {},
   "source": [
    "<div style=\"border-top: 7px solid #800080; animation: sparkling 2s linear infinite;\"></div>\n",
    "\n",
    "<style>\n",
    "@keyframes sparkling {\n",
    "  0% { background-position: 0 0; }\n",
    "  100% { background-position: 100% 0; }\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa068a0",
   "metadata": {},
   "source": [
    "**Part 3**\n",
    "\n",
    "- Train SVM model on debiased dataset (df_train2) with TF-IDF features, and \n",
    "- with Word-Embedding features\n",
    "- Experiment with linear and rbf kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5ab3e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bd231a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for tf-idf based linear SVM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      1.00      0.64      4577\n",
      "           1       0.05      1.00      0.10       496\n",
      "           2       0.26      1.00      0.41      2531\n",
      "           3       0.24      0.65      0.36       144\n",
      "           4       0.25      1.00      0.39      2390\n",
      "           5       0.04      1.00      0.08       413\n",
      "\n",
      "   micro avg       0.21      1.00      0.35     10551\n",
      "   macro avg       0.22      0.94      0.33     10551\n",
      "weighted avg       0.33      1.00      0.48     10551\n",
      " samples avg       0.21      0.50      0.28     10551\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Suppress ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Define and train the classifier with linear kernel\n",
    "clf = OneVsRestClassifier(SVC(kernel='linear', class_weight='balanced', max_iter=1000, random_state=0))\n",
    "clf.fit(X_train2, y_train2) # tf-idf vectors\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred2 = clf.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Classification report for tf-idf based linear SVM:\")\n",
    "print(classification_report(y_test2, y_pred2, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6353c2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for tf-idf based rbf SVM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      1.00      0.64      4577\n",
      "           1       0.05      1.00      0.10       496\n",
      "           2       0.26      1.00      0.41      2531\n",
      "           3       0.54      0.38      0.45       144\n",
      "           4       0.25      1.00      0.39      2390\n",
      "           5       0.04      1.00      0.08       413\n",
      "\n",
      "   micro avg       0.21      0.99      0.35     10551\n",
      "   macro avg       0.27      0.90      0.35     10551\n",
      "weighted avg       0.33      0.99      0.48     10551\n",
      " samples avg       0.21      0.50      0.28     10551\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Suppress ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Define and train the SVM classifier with RBF kernel\n",
    "clf_svm_rbf = OneVsRestClassifier(SVC(kernel='rbf', class_weight='balanced', max_iter=1000, random_state=0))\n",
    "clf_svm_rbf.fit(X_train2, y_train2) \n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred_svm_rbf = clf_svm_rbf.predict(X_test2)\n",
    "\n",
    "# Evaluate the performance\n",
    "print(\"Classification report for tf-idf based rbf SVM:\")\n",
    "print(classification_report(y_test2, y_pred_svm_rbf, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3edef717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for word-embedding based linear SVM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.98      0.64      4577\n",
      "           1       0.05      0.85      0.09       496\n",
      "           2       0.26      0.80      0.39      2531\n",
      "           3       0.01      0.74      0.03       144\n",
      "           4       0.25      0.97      0.39      2390\n",
      "           5       0.04      0.94      0.08       413\n",
      "\n",
      "   micro avg       0.19      0.92      0.31     10551\n",
      "   macro avg       0.18      0.88      0.27     10551\n",
      "weighted avg       0.33      0.92      0.47     10551\n",
      " samples avg       0.19      0.47      0.25     10551\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Suppress ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Define and train the classifier\n",
    "clf_1 = OneVsRestClassifier(SVC(kernel='linear', class_weight='balanced', max_iter=1000, random_state=0))\n",
    "clf_1.fit(X_train_2, y_train_2) # word embeddings\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred_2 = clf_1.predict(X_test_2)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Classification report for word-embedding based linear SVM:\")\n",
    "print(classification_report(y_test_2, y_pred_2, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "258b83c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for word-embedding based rbf SVM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.99      0.64      4577\n",
      "           1       0.05      1.00      0.10       496\n",
      "           2       0.26      1.00      0.41      2531\n",
      "           3       0.01      1.00      0.03       144\n",
      "           4       0.25      1.00      0.39      2390\n",
      "           5       0.04      1.00      0.08       413\n",
      "\n",
      "   micro avg       0.18      1.00      0.31     10551\n",
      "   macro avg       0.18      1.00      0.28     10551\n",
      "weighted avg       0.33      1.00      0.47     10551\n",
      " samples avg       0.18      0.50      0.25     10551\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Suppress ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Define and train the SVM classifier with RBF kernel\n",
    "clf_svm_rbf_1 = OneVsRestClassifier(SVC(kernel='rbf', class_weight='balanced', max_iter=1000, random_state=0))\n",
    "clf_svm_rbf_1.fit(X_train_2, y_train_2)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred_svm_rbf_2 = clf_svm_rbf_1.predict(X_test_2)\n",
    "\n",
    "# Evaluate the performance\n",
    "print(\"Classification report for word-embedding based rbf SVM:\")\n",
    "print(classification_report(y_test_2, y_pred_svm_rbf_2, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82097b9",
   "metadata": {},
   "source": [
    "<div style=\"border-top: 7px solid #800080; animation: sparkling 2s linear infinite;\"></div>\n",
    "\n",
    "<style>\n",
    "@keyframes sparkling {\n",
    "  0% { background-position: 0 0; }\n",
    "  100% { background-position: 100% 0; }\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33edefc",
   "metadata": {},
   "source": [
    "**Part 4**\n",
    "\n",
    "- Hyperparameter tuning for best models of decision trees and SVM\n",
    "- Evaluate the results\n",
    "- Evaluation of the models against the real-world test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf367ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 10\n",
      "Best gamma: 1\n",
      "F1 score: 0.6714457541496271\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Set parameter range\n",
    "parameters = {\n",
    "    'estimator__C': [0.01, 0.1, 1, 10],\n",
    "    'estimator__gamma': [0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm_classifier = SVC(class_weight='balanced', max_iter=1000, random_state=0)\n",
    "\n",
    "# Wrap it with OneVsRestClassifier\n",
    "ovr_classifier = OneVsRestClassifier(svm_classifier)\n",
    "\n",
    "# Perform search\n",
    "grid_search = GridSearchCV(ovr_classifier, parameters, cv=5, scoring='f1_micro')\n",
    "grid_search.fit(X_train2, y_train2) # tf-idf based\n",
    "\n",
    "# Get the best model\n",
    "best_rbf = grid_search.best_estimator_\n",
    "\n",
    "# Make prediction with the best model\n",
    "y_pred_best_rbf = best_rbf.predict(X_test2)\n",
    "\n",
    "# Evaluation\n",
    "f1 = f1_score(y_test2, y_pred_best_rbf,average='micro')\n",
    "\n",
    "print(f'Best C: {best_rbf.estimator.C}')\n",
    "print(f'Best gamma: {best_rbf.estimator.gamma}')\n",
    "print(f'F1 score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d2d7eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: Precision=0.82, Recall=0.75, F1 Score=0.79\n",
      "Class 1: Precision=0.34, Recall=0.43, F1 Score=0.38\n",
      "Class 2: Precision=0.73, Recall=0.73, F1 Score=0.73\n",
      "Class 3: Precision=0.54, Recall=0.23, F1 Score=0.32\n",
      "Class 4: Precision=0.62, Recall=0.54, F1 Score=0.57\n",
      "Class 5: Precision=0.21, Recall=0.40, F1 Score=0.28\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Calculate precision, recall, and F1 score for each class\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test2, y_pred_best_rbf, average=None)\n",
    "\n",
    "# Print precision, recall, and F1 score for each class\n",
    "for i in range(len(precision)):\n",
    "    print(f'Class {i}: Precision={precision[i]:.2f}, Recall={recall[i]:.2f}, F1 Score={f1[i]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9665233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best max_depth: None\n",
      "Best min_samples_split: 5\n",
      "Best min_samples_leaf: 1\n",
      "Best max_features: None\n",
      "F1 score: 0.6000079481778803\n"
     ]
    }
   ],
   "source": [
    "# Set parameter range\n",
    "parameters = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5, 6],\n",
    "    'max_features': [ None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier(class_weight='balanced', random_state=0)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(dt_classifier, parameters, cv=5, scoring='f1_micro')\n",
    "grid_search.fit(X_train2, y_train2)\n",
    "\n",
    "# Get the best model\n",
    "best_dt = grid_search.best_estimator_\n",
    "\n",
    "# Make prediction with the best model\n",
    "y_pred_best_dt = best_dt.predict(X_test2)\n",
    "\n",
    "# Evaluation\n",
    "f1 = f1_score(y_test2, y_pred_best_dt, average='micro')\n",
    "\n",
    "print(f'Best max_depth: {best_dt.max_depth}')\n",
    "print(f'Best min_samples_split: {best_dt.min_samples_split}')\n",
    "print(f'Best min_samples_leaf: {best_dt.min_samples_leaf}')\n",
    "print(f'Best max_features: {best_dt.max_features}')\n",
    "print(f'F1 score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3a88fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: Precision=0.73, Recall=0.76, F1 Score=0.74\n",
      "Class 1: Precision=0.18, Recall=0.52, F1 Score=0.27\n",
      "Class 2: Precision=0.59, Recall=0.78, F1 Score=0.67\n",
      "Class 3: Precision=0.18, Recall=0.42, F1 Score=0.25\n",
      "Class 4: Precision=0.49, Recall=0.70, F1 Score=0.58\n",
      "Class 5: Precision=0.09, Recall=0.31, F1 Score=0.15\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision, recall, and F1 score for each class\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test2, y_pred_best_dt, average=None)\n",
    "\n",
    "# Print precision, recall, and F1 score for each class\n",
    "for i in range(len(precision)):\n",
    "    print(f'Class {i}: Precision={precision[i]:.2f}, Recall={recall[i]:.2f}, F1 Score={f1[i]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81189a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_decision_tree.joblib']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "# Save the best models, in case later use\n",
    "dump(best_rbf, 'best_svm_rbf.joblib')\n",
    "dump(best_dt, 'best_decision_tree.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cb18de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import saved model for validation against real-world data\n",
    "from joblib import load\n",
    "\n",
    "# Load the saved models to validate against real-world test data\n",
    "dt_model = load('best_decision_tree.joblib')\n",
    "svm_model = load('best_svm_rbf.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "467a2b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed test data\n",
    "test_df = pd.read_csv('ready_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca444018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test data with the same TfidfVectorizer\n",
    "X_test = vectorizer.transform(test_df['lemmas'])\n",
    "y_test = test_df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3fb4c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2173226433430515\n",
      "Class 0: Precision=0.18, Recall=0.82, F1 Score=0.30\n",
      "Class 1: Precision=0.02, Recall=0.47, F1 Score=0.05\n",
      "Class 2: Precision=0.16, Recall=0.75, F1 Score=0.26\n",
      "Class 3: Precision=0.03, Recall=0.45, F1 Score=0.06\n",
      "Class 4: Precision=0.13, Recall=0.70, F1 Score=0.22\n",
      "Class 5: Precision=0.03, Recall=0.44, F1 Score=0.05\n"
     ]
    }
   ],
   "source": [
    "# Validation of the decision tree model\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "y_pred_test=dt_model.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred_test, average='micro')\n",
    "print(f1)\n",
    "\n",
    "# Calculate precision, recall, and F1 score for each class\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_test, average=None)\n",
    "\n",
    "# Print precision, recall, and F1 score for each class\n",
    "for i in range(len(precision)):\n",
    "    print(f'Class {i}: Precision={precision[i]:.2f}, Recall={recall[i]:.2f}, F1 Score={f1[i]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d9f77b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3808105995757192\n",
      "Class 0: Precision=0.30, Recall=0.83, F1 Score=0.44\n",
      "Class 1: Precision=0.08, Recall=0.48, F1 Score=0.13\n",
      "Class 2: Precision=0.34, Recall=0.71, F1 Score=0.46\n",
      "Class 3: Precision=0.37, Recall=0.32, F1 Score=0.34\n",
      "Class 4: Precision=0.25, Recall=0.54, F1 Score=0.34\n",
      "Class 5: Precision=0.09, Recall=0.67, F1 Score=0.16\n"
     ]
    }
   ],
   "source": [
    "# Validation of the svm model\n",
    "y_pred_test_svm=svm_model.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred_test_svm, average='micro')\n",
    "print(f1)\n",
    "\n",
    "# Calculate precision, recall, and F1 score for each class\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_test_svm, average=None)\n",
    "\n",
    "# Print precision, recall, and F1 score for each class\n",
    "for i in range(len(precision)):\n",
    "    print(f'Class {i}: Precision={precision[i]:.2f}, Recall={recall[i]:.2f}, F1 Score={f1[i]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81fa102",
   "metadata": {},
   "source": [
    "<div style=\"border-top: 7px solid #800080; animation: sparkling 2s linear infinite;\"></div>\n",
    "\n",
    "<style>\n",
    "@keyframes sparkling {\n",
    "  0% { background-position: 0 0; }\n",
    "  100% { background-position: 100% 0; }\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cd9578",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "- Decision Trees on Original vs. Debiased Dataset:\n",
    "\n",
    "Models: Decision Trees trained on both the original dataset and the dataset after applying a debias method.\n",
    "Purpose: Evaluate the impact of the debias method on the performance of Decision Tree models.\n",
    "\n",
    "- Decision Trees with TF-IDF vs. Word Embedding Features:\n",
    "\n",
    "Models: Decision Tree models trained using both TF-IDF and word embedding feature representations.\n",
    "Purpose: Compare the performance of Decision Tree models when using different feature representations.\n",
    "\n",
    "- SVM Models with Linear and RBF Kernel on TF-IDF vs. Word Embedding Features:\n",
    "\n",
    "Models: SVM models trained with both linear and RBF kernels, using TF-IDF and word embedding features.\n",
    "Purpose: Compare the performance of SVM models with different kernels and feature representations.\n",
    "\n",
    "- Hyperparameter Tuning for Best Models:\n",
    "\n",
    "Purpose: Perform hyperparameter tuning for the best-performing models obtained from the previous experiments to evaluate potential improvements in model performance.\n",
    "\n",
    "**Result:**\n",
    "- The debias method has minimal impact on model performance.\n",
    "- Decision Tree models exhibit superior performance when utilising TF-IDF features.\n",
    "- SVM models demonstrate better performance when employing the RBF kernel and utilizing TF-IDF features.\n",
    "- Hyperparameter tuning significantly improves model performance. However, for classes with lower frequencies, the models still struggle to accurately capture patterns during training and perform pooly on real-world data. SVM performs better with real data than decision trees.\n",
    "\n",
    "**Decision for Next Step:**\n",
    "- Design and implement neural networks to compare the performance with the existing models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
